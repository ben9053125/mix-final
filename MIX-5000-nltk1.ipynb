{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as components\n",
    "from kfp.components import func_to_container_op, InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_sample_dowload_and_preprocess(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    import re\n",
    "    import string\n",
    "    import pandas as pd\n",
    "    from random import shuffle\n",
    "    import nltk\n",
    "    import joblib\n",
    "    from nltk.corpus import twitter_samples\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tqdm import tqdm\n",
    "    from nltk import data\n",
    "    \n",
    "    data.path.append(log_folder)\n",
    "    nltk.download('twitter_samples', download_dir = log_folder)\n",
    "    nltk.download('stopwords', download_dir = log_folder)\n",
    "    \n",
    "    pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    print(f\"positive sentiment GOOD total samples {len(pos_tweets)}\")\n",
    "    print(f\"negative sentiment  Bad total samples {len(neg_tweets)}\")\n",
    "    \n",
    "    class Twitter_Preprocess():\n",
    "    \n",
    "        def __init__(self):\n",
    "            self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                                           reduce_len=True)\n",
    "            self.stopwords_en = stopwords.words('english') \n",
    "            self.punctuation_en = string.punctuation\n",
    "            self.stemmer = PorterStemmer() \n",
    "\n",
    "        def __remove_unwanted_characters__(self, tweet):\n",
    "            tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "            tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "            tweet = re.sub(r'#', '', tweet)\n",
    "            tweet = re.sub('\\S+@\\S+', '', tweet)\n",
    "            tweet = re.sub(r'\\d+', '', tweet)\n",
    "            return tweet\n",
    "\n",
    "        def __tokenize_tweet__(self, tweet):        \n",
    "            return self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        def __remove_stopwords__(self, tweet_tokens):\n",
    "            tweets_clean = []\n",
    "\n",
    "            for word in tweet_tokens:\n",
    "                if (word not in self.stopwords_en and \n",
    "                    word not in self.punctuation_en):\n",
    "                    tweets_clean.append(word)\n",
    "            return tweets_clean\n",
    "\n",
    "        def __text_stemming__(self,tweet_tokens):\n",
    "            tweets_stem = [] \n",
    "\n",
    "            for word in tweet_tokens:\n",
    "                stem_word = self.stemmer.stem(word)  \n",
    "                tweets_stem.append(stem_word)\n",
    "            return tweets_stem\n",
    "\n",
    "        def preprocess(self, tweets):\n",
    "            tweets_processed = []\n",
    "            for _, tweet in tqdm(enumerate(tweets)):        \n",
    "                tweet = self.__remove_unwanted_characters__(tweet)            \n",
    "                tweet_tokens = self.__tokenize_tweet__(tweet)            \n",
    "                tweet_clean = self.__remove_stopwords__(tweet_tokens)\n",
    "                tweet_stems = self.__text_stemming__(tweet_clean)\n",
    "                tweets_processed.extend([tweet_stems])\n",
    "            return tweets_processed\n",
    "    \n",
    "    twitter_text_processor = Twitter_Preprocess()\n",
    "    processed_pos_tweets = twitter_text_processor.preprocess(pos_tweets)\n",
    "    processed_neg_tweets = twitter_text_processor.preprocess(neg_tweets)\n",
    "    \n",
    "    def build_bow_dict(tweets, labels):\n",
    "        freq = {}\n",
    "        for tweet, label in list(zip(tweets, labels)):\n",
    "            for word in tweet:\n",
    "                freq[(word, label)] = freq.get((word, label), 0) + 1    \n",
    "        return freq\n",
    "\n",
    "    \n",
    "    \n",
    "    labels = [1 for i in range(len(processed_pos_tweets))]\n",
    "    labels.extend([0 for i in range(len(processed_neg_tweets))])\n",
    "    \n",
    "    twitter_processed_corpus = processed_pos_tweets + processed_neg_tweets\n",
    "    bow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)\n",
    "    \n",
    "    shuffle(processed_pos_tweets)\n",
    "    shuffle(processed_neg_tweets)\n",
    "\n",
    "    positive_tweet_label = [1 for i in processed_pos_tweets]\n",
    "    negative_tweet_label = [0 for i in processed_neg_tweets]\n",
    "\n",
    "    tweet_df = pd.DataFrame(list(zip(twitter_processed_corpus,\n",
    "                            positive_tweet_label+negative_tweet_label)),\n",
    "                            columns=[\"processed_tweet\", \"label\"])\n",
    "\n",
    "    train_X_tweet, test_X_tweet, train_Y, test_Y = train_test_split(tweet_df[\"processed_tweet\"],\n",
    "                                                                    tweet_df[\"label\"],\n",
    "                                                                    test_size = 0.20,\n",
    "                                                                    stratify=tweet_df[\"label\"])\n",
    "    \n",
    "    print(f\"train_X_tweet {train_X_tweet.shape}, test_X_tweet {test_X_tweet.shape}\")\n",
    "    print(f\"train_Y {train_Y.shape}, test_Y {test_Y.shape}\")\n",
    "    \n",
    "    joblib.dump(bow_word_frequency, log_folder + '/bow_word_frequency.pkl')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    joblib.dump(train_X_tweet, log_folder + '/train_X_tweet.pkl')\n",
    "    joblib.dump(test_X_tweet, log_folder + '/test_X_tweet.pkl')\n",
    "    joblib.dump(train_Y, log_folder + '/train_Y.pkl')\n",
    "    joblib.dump(test_Y, log_folder + '/test_Y.pkl')\n",
    "    \n",
    "    return ([log_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cornell_dowload_and_preprocess(log_folder:str) -> NamedTuple('Outputs', [('logdir2',str)]):\n",
    "    import re\n",
    "    import string\n",
    "    import pandas as pd\n",
    "    from random import shuffle\n",
    "    import nltk\n",
    "    import joblib\n",
    "    from nltk.corpus import movie_reviews\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tqdm import tqdm\n",
    "    from nltk import data\n",
    "    \n",
    "    data.path.append(log_folder)\n",
    "    nltk.download('movie_reviews', download_dir = log_folder)\n",
    "    nltk.download('stopwords', download_dir = log_folder)\n",
    "    \n",
    "    pos_tweets = []\n",
    "    neg_tweets = []\n",
    "    \n",
    "    for fileid in movie_reviews.fileids('pos'):\n",
    "        content = ''\n",
    "        for word in movie_reviews.words(fileid):\n",
    "            content += ' ' + word\n",
    "        pos_tweets.append(content)\n",
    "    for fileid in movie_reviews.fileids('neg'):\n",
    "        content = ''\n",
    "        for word in movie_reviews.words(fileid):\n",
    "            content += ' ' + word\n",
    "        neg_tweets.append(content)\n",
    "\n",
    "    print(f\"positive sentiment GOOD total samples {len(pos_tweets)}\")\n",
    "    print(f\"negative sentiment  Bad total samples {len(neg_tweets)}\")\n",
    "    \n",
    "    class Twitter_Preprocess():\n",
    "    \n",
    "        def __init__(self):\n",
    "            self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                                           reduce_len=True)\n",
    "            self.stopwords_en = stopwords.words('english') \n",
    "            self.punctuation_en = string.punctuation\n",
    "            self.stemmer = PorterStemmer() \n",
    "\n",
    "        def __remove_unwanted_characters__(self, tweet):\n",
    "            tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "            tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "            tweet = re.sub(r'#', '', tweet)\n",
    "            tweet = re.sub('\\S+@\\S+', '', tweet)\n",
    "            tweet = re.sub(r'\\d+', '', tweet)\n",
    "            return tweet\n",
    "\n",
    "        def __tokenize_tweet__(self, tweet):        \n",
    "            return self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        def __remove_stopwords__(self, tweet_tokens):\n",
    "            tweets_clean = []\n",
    "\n",
    "            for word in tweet_tokens:\n",
    "                if (word not in self.stopwords_en and \n",
    "                    word not in self.punctuation_en):\n",
    "                    tweets_clean.append(word)\n",
    "            return tweets_clean\n",
    "\n",
    "        def __text_stemming__(self,tweet_tokens):\n",
    "            tweets_stem = [] \n",
    "\n",
    "            for word in tweet_tokens:\n",
    "                stem_word = self.stemmer.stem(word)  \n",
    "                tweets_stem.append(stem_word)\n",
    "            return tweets_stem\n",
    "\n",
    "        def preprocess(self, tweets):\n",
    "            tweets_processed = []\n",
    "            for _, tweet in tqdm(enumerate(tweets)):        \n",
    "                tweet = self.__remove_unwanted_characters__(tweet)            \n",
    "                tweet_tokens = self.__tokenize_tweet__(tweet)            \n",
    "                tweet_clean = self.__remove_stopwords__(tweet_tokens)\n",
    "                tweet_stems = self.__text_stemming__(tweet_clean)\n",
    "                tweets_processed.extend([tweet_stems])\n",
    "            return tweets_processed\n",
    "    \n",
    "    twitter_text_processor = Twitter_Preprocess()\n",
    "    processed_pos_tweets = twitter_text_processor.preprocess(pos_tweets)\n",
    "    processed_neg_tweets = twitter_text_processor.preprocess(neg_tweets)\n",
    "    \n",
    "    def build_bow_dict(tweets, labels):\n",
    "        freq = {}\n",
    "        for tweet, label in list(zip(tweets, labels)):\n",
    "            for word in tweet:\n",
    "                freq[(word, label)] = freq.get((word, label), 0) + 1    \n",
    "        return freq\n",
    "\n",
    "    labels = [1 for i in range(len(processed_pos_tweets))]\n",
    "    labels.extend([0 for i in range(len(processed_neg_tweets))])\n",
    "    \n",
    "    twitter_processed_corpus = processed_pos_tweets + processed_neg_tweets\n",
    "    bow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)\n",
    "    \n",
    "    shuffle(processed_pos_tweets)\n",
    "    shuffle(processed_neg_tweets)\n",
    "\n",
    "    positive_tweet_label = [1 for i in processed_pos_tweets]\n",
    "    negative_tweet_label = [0 for i in processed_neg_tweets]\n",
    "\n",
    "    tweet_df = pd.DataFrame(list(zip(twitter_processed_corpus,\n",
    "                            positive_tweet_label+negative_tweet_label)),\n",
    "                            columns=[\"processed_tweet\", \"label\"])\n",
    "    \n",
    "    train_X_tweet, test_X_tweet, train_Y, test_Y = train_test_split(tweet_df[\"processed_tweet\"],\n",
    "                                                                    tweet_df[\"label\"],\n",
    "                                                                    test_size = 0.20,\n",
    "                                                                    stratify=tweet_df[\"label\"])\n",
    "    \n",
    "    print(f\"train_X_tweet {train_X_tweet.shape}, test_X_tweet {test_X_tweet.shape}\")\n",
    "    print(f\"train_Y {train_Y.shape}, test_Y {test_Y.shape}\")\n",
    "    \n",
    "    joblib.dump(bow_word_frequency, log_folder + '/bow_word_frequency_C.pkl')\n",
    "    joblib.dump(train_X_tweet, log_folder + '/train_X_tweet_C.pkl')\n",
    "    joblib.dump(test_X_tweet, log_folder + '/test_X_tweet_C.pkl')\n",
    "    joblib.dump(train_Y, log_folder + '/train_Y_C.pkl')\n",
    "    joblib.dump(test_Y, log_folder + '/test_Y_C.pkl')\n",
    "    \n",
    "    return ([log_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_process(log_folder:str,log_folder2:str) -> NamedTuple('Outputs', [('logdir',str),('logdir2',str), ('numpydir',str)]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    \n",
    "    bow_word_frequency = joblib.load(open(log_folder + '/bow_word_frequency.pkl','rb'))\n",
    "    train_X_tweet = joblib.load(open(log_folder + '/train_X_tweet.pkl','rb'))\n",
    "    test_X_tweet = joblib.load(open(log_folder + '/test_X_tweet.pkl','rb'))\n",
    "    train_Y = joblib.load(open(log_folder + '/train_Y.pkl','rb'))\n",
    "    test_Y = joblib.load(open(log_folder + '/test_Y.pkl','rb'))\n",
    "     \n",
    "    C_bow_word_frequency = joblib.load(open(log_folder + '/bow_word_frequency_C.pkl','rb'))\n",
    "    C_train_X_tweet = joblib.load(open(log_folder + '/train_X_tweet_C.pkl','rb'))\n",
    "    C_test_X_tweet = joblib.load(open(log_folder + '/test_X_tweet_C.pkl','rb'))\n",
    "    C_train_Y = joblib.load(open(log_folder + '/train_Y_C.pkl','rb'))\n",
    "    C_test_Y = joblib.load(open(log_folder + '/test_Y_C.pkl','rb'))\n",
    "    \n",
    "    def extract_features(processed_tweet, bow_word_frequency):\n",
    "        features = np.zeros((1,3))\n",
    "        features[0,0] = 1\n",
    "\n",
    "        for word in processed_tweet:\n",
    "            features[0,1] = bow_word_frequency.get((word, 1), 0)+features[0,1]\n",
    "            features[0,2] = bow_word_frequency.get((word, 0), 0)+features[0,2]\n",
    "        return features\n",
    "    def C_extract_features(processed_tweet, bow_word_frequency):\n",
    "        features = np.zeros((1,3))\n",
    "        features[0,0] = 1\n",
    "\n",
    "        for word in processed_tweet:\n",
    "            features[0,1] = bow_word_frequency.get((word, 1), 0)+features[0,1]\n",
    "            features[0,2] = bow_word_frequency.get((word, 0), 0)+features[0,2]\n",
    "        return features\n",
    "    \n",
    "    train_X = np.zeros((len(train_X_tweet), 3))\n",
    "    for index, tweet in enumerate(train_X_tweet):\n",
    "        train_X[index, :] = extract_features(tweet, bow_word_frequency)\n",
    "\n",
    "    test_X = np.zeros((len(test_X_tweet), 3))\n",
    "    for index, tweet in enumerate(test_X_tweet):\n",
    "        test_X[index, :] = extract_features(tweet, bow_word_frequency)\n",
    "    \n",
    "    C_train_X = np.zeros((len(C_train_X_tweet), 3))\n",
    "    for index, tweet in enumerate(C_train_X_tweet):\n",
    "        C_train_X[index, :] = C_extract_features(tweet, C_bow_word_frequency)\n",
    "\n",
    "    C_test_X = np.zeros((len(C_test_X_tweet), 3))\n",
    "    for index, tweet in enumerate(C_test_X_tweet):\n",
    "        C_test_X[index, :] = C_extract_features(tweet, C_bow_word_frequency)\n",
    "    \n",
    "    \n",
    "    print(f\"train_X {train_X.shape}, test_X {test_X.shape}\")\n",
    "    print(f\"C_train_X {C_train_X.shape}, C_test_X {C_test_X.shape}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not os.path.isdir(log_folder + '/numpy'):\n",
    "        os.makedirs(log_folder + '/numpy')\n",
    "    numpy_folder = log_folder + '/numpy'\n",
    "    joblib.dump(train_X, numpy_folder + '/train_X.pkl')\n",
    "    joblib.dump(test_X, numpy_folder + '/test_X.pkl')\n",
    "    joblib.dump(C_train_X, numpy_folder + '/train_X_C.pkl')\n",
    "    joblib.dump(C_test_X, numpy_folder + '/test_X_C.pkl')\n",
    "    \n",
    "    return ([log_folder,log_folder, numpy_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_logistic(log_folder:str, numpy_folder:str)->NamedTuple('Outputs',[('logdir',str), ('sklearndir',str), ('sklearnscore',float), ('C_sklearnscore',float)]):\n",
    "    \n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    \n",
    "    train_X = joblib.load(open(numpy_folder + '/train_X.pkl','rb'))\n",
    "    test_X = joblib.load(open(numpy_folder + '/test_X.pkl','rb'))\n",
    "    train_Y = joblib.load(open(log_folder + '/train_Y.pkl','rb'))\n",
    "    test_Y = joblib.load(open(log_folder + '/test_Y.pkl','rb'))\n",
    "    \n",
    "    C_train_X = joblib.load(open(numpy_folder + '/train_X_C.pkl','rb'))\n",
    "    C_test_X = joblib.load(open(numpy_folder + '/test_X_C.pkl','rb'))\n",
    "    C_train_Y = joblib.load(open(log_folder + '/train_Y_C.pkl','rb'))\n",
    "    C_test_Y = joblib.load(open(log_folder + '/test_Y_C.pkl','rb'))\n",
    "    \n",
    "    \n",
    "    clf = SGDClassifier(loss='log')\n",
    "    clf.fit(train_X, np.array(train_Y).reshape(-1,1))\n",
    "    y_pred = clf.predict(test_X)\n",
    "    y_pred_probs = clf.predict(test_X)\n",
    "    \n",
    "        \n",
    "    C_clf = SGDClassifier(loss='log')\n",
    "    C_clf.fit(C_train_X, np.array(C_train_Y).reshape(-1,1))\n",
    "    C_y_pred = clf.predict(C_test_X)\n",
    "    C_y_pred_probs = clf.predict(C_test_X)\n",
    "  \n",
    "    print(f\"Scikit learn logistic regression accuracy is {accuracy_score(test_Y , y_pred)*100:.2f}\")\n",
    "    print(f\"Scikit learn logistic regression accuracy is {accuracy_score(C_test_Y , C_y_pred)*100:.2f}\")\n",
    "    \n",
    "    if not os.path.isdir(numpy_folder + '/sklearn'):\n",
    "        os.makedirs(numpy_folder + '/sklearn')\n",
    "    sklearn_folder = numpy_folder + '/sklearn'\n",
    "    joblib.dump(clf, sklearn_folder + '/sklearn.pkl')\n",
    "    joblib.dump(C_clf, sklearn_folder + '/sklearn_C.pkl')\n",
    "    \n",
    "    sklearn_score = accuracy_score(test_Y , y_pred)\n",
    "    C_sklearn_score = accuracy_score(C_test_Y , C_y_pred)\n",
    "    \n",
    "    return ([log_folder, sklearn_folder, sklearn_score, C_sklearn_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_svm(log_folder:str, numpy_folder:str)->NamedTuple('Outputs',[('logdir',str),('svmdir',str),('svm_score',float),('C_svm_score',float)]):\n",
    "  import os\n",
    "  import joblib\n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  import nltk\n",
    "  from nltk.tokenize import TweetTokenizer\n",
    "  from sklearn.pipeline import make_pipeline, Pipeline\n",
    "  nltk.download('stopwords')\n",
    "  from nltk.corpus import stopwords\n",
    "  from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "  from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "  from sklearn.pipeline import make_pipeline, Pipeline\n",
    "  from sklearn.model_selection import GridSearchCV\n",
    "  from sklearn.metrics import roc_curve, auc\n",
    "  from sklearn.svm import SVC\n",
    "  from sklearn.metrics import accuracy_score\n",
    "\n",
    "  X_train = joblib.load(open(numpy_folder + '/train_X.pkl','rb'))\n",
    "  X_test = joblib.load(open(numpy_folder + '/test_X.pkl','rb'))\n",
    "  y_train = joblib.load(open(log_folder + '/train_Y.pkl','rb'))\n",
    "  y_test = joblib.load(open(log_folder + '/test_Y.pkl','rb'))\n",
    "    \n",
    "  C_X_train = joblib.load(open(numpy_folder + '/train_X_C.pkl','rb'))\n",
    "  C_X_test = joblib.load(open(numpy_folder + '/test_X_C.pkl','rb'))\n",
    "  C_y_train = joblib.load(open(log_folder + '/train_Y_C.pkl','rb'))\n",
    "  C_y_test = joblib.load(open(log_folder + '/test_Y_C.pkl','rb'))\n",
    "\n",
    "  grid_svm = SVC(kernel='linear', C=1.0)\n",
    "  svm_fit = grid_svm.fit(X_train, np.array(y_train).reshape(-1,1))\n",
    "  y_pred = grid_svm.predict(X_test)\n",
    "  svm_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "  C_grid_svm = SVC(kernel='linear', C=1.0)\n",
    "  C_svm_fit = grid_svm.fit(C_X_train, np.array(C_y_train).reshape(-1,1))\n",
    "  C_y_pred = grid_svm.predict(C_X_test)\n",
    "  C_svm_score = accuracy_score(C_y_test, C_y_pred)\n",
    "    \n",
    "  \n",
    "  if not os.path.isdir(numpy_folder + '/svm'):\n",
    "    os.makedirs(numpy_folder + '/svm')\n",
    "  svm_folder = numpy_folder + '/svm'\n",
    "  joblib.dump(svm_fit, svm_folder + '/svm.pkl')\n",
    "  joblib.dump(C_svm_fit, svm_folder + '/svm_C.pkl')\n",
    "\n",
    "    \n",
    "  return ([log_folder,svm_folder,svm_score,C_svm_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(log_folder:str, numpy_folder:str) -> NamedTuple('Outputs', [('logdir',str), ('logisticdir',str), ('logisticscore',float), ('C_logisticscore',float)]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    \n",
    "    train_X = joblib.load(open(numpy_folder + '/train_X.pkl','rb'))\n",
    "    test_X = joblib.load(open(numpy_folder + '/test_X.pkl','rb'))\n",
    "    train_Y = joblib.load(open(log_folder + '/train_Y.pkl','rb'))\n",
    "    test_Y = joblib.load(open(log_folder + '/test_Y.pkl','rb'))\n",
    "        \n",
    "    C_train_X = joblib.load(open(numpy_folder + '/train_X_C.pkl','rb'))\n",
    "    C_test_X = joblib.load(open(numpy_folder + '/test_X_C.pkl','rb'))\n",
    "    C_train_Y = joblib.load(open(log_folder + '/train_Y_C.pkl','rb'))\n",
    "    C_test_Y = joblib.load(open(log_folder + '/test_Y_C.pkl','rb'))\n",
    "    def sigmoid(z): \n",
    "        h = 1 / (1+ np.exp(-z))\n",
    "        return h\n",
    "    \n",
    "    def gradientDescent(x, y, theta, alpha, num_iters, c):\n",
    "        m = x.shape[0]\n",
    "        for i in range(0, num_iters):\n",
    "            z = np.dot(x, theta)\n",
    "            h = sigmoid(z)\n",
    "            J = (-1/m) * ((np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h))) + (c * np.sum(theta)))\n",
    "            theta = theta - (alpha / m) * np.dot((x.T), (h - y))\n",
    "            J = float(J)\n",
    "        return J, theta\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    J, theta = gradientDescent(train_X, np.array(train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000, 0.1)\n",
    "    C_J, C_theta = gradientDescent(C_train_X, np.array(C_train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000, 0.1)\n",
    "    \n",
    "    print(f\"The cost after training is {J:.8f}.\")\n",
    "    print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n",
    "    print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(C_theta)]}\")\n",
    "    \n",
    "    def predict_tweet(x, theta):\n",
    "        y_pred = sigmoid(np.dot(x, theta))\n",
    "        return y_pred\n",
    "    \n",
    "    predicted_probs = predict_tweet(test_X, theta)\n",
    "    predicted_labels = np.where(predicted_probs > 0.5, 1, 0)\n",
    "    print(f\"Own implementation of logistic regression accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) / len(test_Y)*100:.2f}\")\n",
    "    \n",
    "    C_predicted_probs = predict_tweet(C_test_X, C_theta)\n",
    "    C_predicted_labels = np.where(C_predicted_probs > 0.5, 1, 0)\n",
    "    print(f\"C_Own implementation of logistic regression accuracy is {len(C_predicted_labels[C_predicted_labels == np.array(C_test_Y).reshape(-1,1)]) / len(C_test_Y)*100:.2f}\")\n",
    "    \n",
    "    if not os.path.isdir(numpy_folder + '/logistic'):\n",
    "        os.makedirs(numpy_folder + '/logistic')\n",
    "    logistic_folder = numpy_folder + '/logistic'\n",
    "    joblib.dump(theta, logistic_folder + '/logistic.pkl')\n",
    "    joblib.dump(C_theta, logistic_folder + '/logistic_C.pkl')\n",
    "    \n",
    "    \n",
    "    \n",
    "    logistic_score = len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) / len(test_Y)\n",
    "    C_logistic_score = len(C_predicted_labels[C_predicted_labels == np.array(C_test_Y).reshape(-1,1)]) / len(C_test_Y)\n",
    "    \n",
    "    return ([log_folder, logistic_folder, logistic_score, C_logistic_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_process_logistic(log_folder:str,og_folder2:str) -> NamedTuple('Outputs', [('logdir',str), ('torchdir',str),  ('torchscore',float),  ('C_torchscore',float)]):\n",
    "    \n",
    "    import torch\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    bow_word_frequency = joblib.load(open(log_folder + '/bow_word_frequency.pkl','rb'))\n",
    "    train_X_tweet = joblib.load(open(log_folder + '/train_X_tweet.pkl','rb'))\n",
    "    test_X_tweet = joblib.load(open(log_folder + '/test_X_tweet.pkl','rb'))\n",
    "    train_Y = joblib.load(open(log_folder + '/train_Y.pkl','rb'))\n",
    "    test_Y = joblib.load(open(log_folder + '/test_Y.pkl','rb'))\n",
    "    C_bow_word_frequency = joblib.load(open(log_folder + '/bow_word_frequency_C.pkl','rb'))\n",
    "    C_train_X_tweet = joblib.load(open(log_folder + '/train_X_tweet_C.pkl','rb'))\n",
    "    C_test_X_tweet = joblib.load(open(log_folder + '/test_X_tweet_C.pkl','rb'))\n",
    "    C_train_Y = joblib.load(open(log_folder + '/train_Y_C.pkl','rb'))\n",
    "    C_test_Y = joblib.load(open(log_folder + '/test_Y_C.pkl','rb'))\n",
    "    \n",
    "    def extract_features(processed_tweet, bow_word_frequency):\n",
    "        features = torch.zeros((1,3))\n",
    "        features[0,0] = 1\n",
    "\n",
    "        for word in processed_tweet:\n",
    "            features[0,1] = bow_word_frequency.get((word, 1), 0) + features[0,1]\n",
    "            features[0,2] = bow_word_frequency.get((word, 0), 0) + features[0,2]\n",
    "        return features\n",
    "    \n",
    "    train_X_Tensor = torch.zeros((len(train_X_tweet), 3))\n",
    "    for index, tweet in enumerate(train_X_tweet):\n",
    "        train_X_Tensor[index, :] = extract_features(tweet, bow_word_frequency)\n",
    "\n",
    "    test_X_Tensor = torch.zeros((len(test_X_tweet), 3))\n",
    "    for index, tweet in enumerate(test_X_tweet):\n",
    "        test_X_Tensor[index, :] = extract_features(tweet, bow_word_frequency)\n",
    "\n",
    "    print(f\"train_X_Tensor {train_X_Tensor.shape}, test_X_Tensor {test_X_Tensor.shape}\")\n",
    "    type(train_X_Tensor)\n",
    "      \n",
    "    C_train_X_Tensor = torch.zeros((len(C_train_X_tweet), 3))\n",
    "    for index, tweet in enumerate(C_train_X_tweet):\n",
    "        C_train_X_Tensor[index, :] = extract_features(tweet, C_bow_word_frequency)\n",
    "\n",
    "    C_test_X_Tensor = torch.zeros((len(C_test_X_tweet), 3))\n",
    "    for index, tweet in enumerate(C_test_X_tweet):\n",
    "        C_test_X_Tensor[index, :] = extract_features(tweet, C_bow_word_frequency)\n",
    "\n",
    "    print(f\"C_train_X_Tensor {C_train_X_Tensor.shape}, test_X_Tensor {C_test_X_Tensor.shape}\")\n",
    "    type(C_train_X_Tensor)\n",
    "      \n",
    "    def sigmoid(z):\n",
    "        h = 1 / (1+ torch.exp(-z))\n",
    "        return h\n",
    "    \n",
    "    def gradientDescent(x, y, theta, alpha, num_iters, c):\n",
    "\n",
    "        m = x.shape[0]\n",
    "\n",
    "        for i in range(0, num_iters):\n",
    "            z = torch.mm(x, theta)\n",
    "            h = sigmoid(z)\n",
    "            J = (-1/m) * ((torch.mm(y.T,torch.log(h)) + torch.mm((1 - y).T, torch.log(1-h))) \n",
    "                          + (c * torch.sum(theta)))\n",
    "            theta = theta - (alpha / m) * torch.mm((x.T), (h - y))\n",
    "            J = float(J)\n",
    "        return J, theta\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    J, theta = gradientDescent(train_X_Tensor,\n",
    "                               torch.reshape(torch.Tensor(train_Y.to_numpy()),(-1,1)),\n",
    "                               torch.zeros((3,1)),1e-7,1000,0.1)\n",
    "    print(f\"The cost after training is {J:.8f}.\")\n",
    "    C_J, C_theta = gradientDescent(C_train_X_Tensor,\n",
    "                               torch.reshape(torch.Tensor(C_train_Y.to_numpy()),(-1,1)),\n",
    "                               torch.zeros((3,1)),1e-7,1000,0.1)\n",
    "    print(f\"C_The cost after training is {J:.8f}.\")\n",
    "  \n",
    "    \n",
    "    def predict_tweet(x,theta):\n",
    "        y_pred = sigmoid(torch.mm(x,theta))\n",
    "        return y_pred\n",
    "    \n",
    "    predicted_probs =predict_tweet(test_X_Tensor, theta)\n",
    "    prediceted_probs=torch.tensor(predicted_probs)\n",
    "    predicted_labels = torch.where(predicted_probs >0.5, torch.tensor(1), torch.tensor(0))\n",
    "    print(f\"Pytorch of logistic regression accuracy is {len(predicted_labels[predicted_labels == torch.reshape(torch.Tensor(test_Y.to_numpy()),(-1,1))]) / len(test_Y)*100:.2f}\")\n",
    "    \n",
    "    C_predicted_probs =predict_tweet(C_test_X_Tensor, C_theta)\n",
    "    C_prediceted_probs=torch.tensor(C_predicted_probs)\n",
    "    C_predicted_labels = torch.where(C_predicted_probs >0.5, torch.tensor(1), torch.tensor(0))\n",
    "    print(f\"C_Pytorch of logistic regression accuracy is {len(C_predicted_labels[C_predicted_labels == torch.reshape(torch.Tensor(C_test_Y.to_numpy()),(-1,1))]) / len(C_test_Y)*100:.2f}\")\n",
    " \n",
    "    \n",
    "    if not os.path.isdir(log_folder + '/torch'):\n",
    "        os.makedirs(log_folder + '/torch')\n",
    "    torch_folder = log_folder + '/torch'\n",
    "    joblib.dump(theta, torch_folder + '/torch.pkl')\n",
    "    joblib.dump(C_theta, torch_folder + '/torch_C.pkl')\n",
    "    \n",
    "    torch_score = len(predicted_labels[predicted_labels == torch.reshape(torch.Tensor(test_Y.to_numpy()),(-1,1))]) / len(test_Y)\n",
    "    C_torch_score = len(C_predicted_labels[C_predicted_labels == torch.reshape(torch.Tensor(C_test_Y.to_numpy()),(-1,1))]) / len(C_test_Y)\n",
    "    \n",
    "    return ([log_folder, torch_folder, torch_score, C_torch_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(sklearn_score:float,logistic_score:float,svm_score:float,torch_score:float,C_sklearn_score:float,C_logistic_score:float,C_svm_score:float,C_torch_score:float) -> NamedTuple('Outputs', [('mlpipeline_metrics', 'Metrics')]):\n",
    "  import json\n",
    "\n",
    "  metrics = {\n",
    "    'metrics': [\n",
    "        {\n",
    "          'name': 'TS_sklearn_score',\n",
    "          'numberValue':  sklearn_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "          'name': 'TS_logistic_score',\n",
    "          'numberValue':  logistic_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "          'name': 'TS_svm_score',\n",
    "          'numberValue':  svm_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "          'name': 'TS_torch_score',\n",
    "          'numberValue':  torch_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "          'name': 'CN_sklearn_score',\n",
    "          'numberValue':  C_sklearn_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "          'name': 'CN_logistic_score',\n",
    "          'numberValue':  C_logistic_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "          'name': 'CN_svm_score',\n",
    "          'numberValue':  C_svm_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "          'name': 'CN_torch_score',\n",
    "          'numberValue':  C_torch_score,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        }\n",
    "    ]\n",
    "  }\n",
    "  return [json.dumps(metrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-259-7b6628410b8a>, line 153)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-259-7b6628410b8a>\"\u001b[0;36m, line \u001b[0;32m153\u001b[0m\n\u001b[0;31m    C_my_prediction_np = C_my_prediction_np,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def http_port(log_folder:str, sklearn_folder:str, svm_folder:str,logistic_folder:str, torch_folder:str):\n",
    "    \n",
    "    import re\n",
    "    import string\n",
    "    import pandas as pd\n",
    "    from random import shuffle\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    import joblib\n",
    "    from nltk.corpus import twitter_samples\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tqdm import tqdm\n",
    "    from nltk import data\n",
    "    from flask import Flask,render_template,url_for,request\n",
    "    \n",
    "    data.path.append(log_folder)\n",
    "\n",
    "    app = Flask(__name__,template_folder='/http-port/templates')\n",
    "\n",
    "    @app.route('/')\n",
    "    def home():\n",
    "        return render_template('home.html')\n",
    "\n",
    "    @app.route('/predict', methods=['POST'])\n",
    "    def predict():\n",
    "\n",
    "        class Preprocess():   \n",
    "            def __init__(self):\n",
    "                self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "                self.stopwords_en = stopwords.words('english') \n",
    "                self.punctuation_en = string.punctuation\n",
    "                self.stemmer = PorterStemmer()        \n",
    "            def __remove_unwanted_characters__(self, tweet):\n",
    "                tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "                tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "                tweet = re.sub(r'#', '', tweet)\n",
    "                tweet = re.sub('\\S+@\\S+', '', tweet)\n",
    "                tweet = re.sub(r'\\d+', '', tweet)\n",
    "                return tweet    \n",
    "            def __tokenize_tweet__(self, tweet):        \n",
    "                return self.tokenizer.tokenize(tweet)   \n",
    "            def __remove_stopwords__(self, tweet_tokens):\n",
    "                tweets_clean = []\n",
    "                for word in tweet_tokens:\n",
    "                    if (word not in self.stopwords_en and \n",
    "                        word not in self.punctuation_en):\n",
    "                        tweets_clean.append(word)\n",
    "                return tweets_clean   \n",
    "            def __text_stemming__(self,tweet_tokens):\n",
    "                tweets_stem = [] \n",
    "                for word in tweet_tokens:\n",
    "                    stem_word = self.stemmer.stem(word)  \n",
    "                    tweets_stem.append(stem_word)\n",
    "                return tweets_stem\n",
    "            def preprocess(self, tweets):\n",
    "                tweets_processed = []\n",
    "                for _, tweet in tqdm(enumerate(tweets)):        \n",
    "                    tweet = self.__remove_unwanted_characters__(tweet)            \n",
    "                    tweet_tokens = self.__tokenize_tweet__(tweet)            \n",
    "                    tweet_clean = self.__remove_stopwords__(tweet_tokens)\n",
    "                    tweet_stems = self.__text_stemming__(tweet_clean)\n",
    "                    tweets_processed.extend([tweet_stems])\n",
    "                return tweets_processed\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def extract_features(processed_tweet, bow_word_frequency):\n",
    "            features = np.zeros((1,3))\n",
    "            features[0,0] = 1\n",
    "            for word in processed_tweet:\n",
    "                features[0,1] = bow_word_frequency.get((word, 1), 0) + features[0,1]\n",
    "                features[0,2] = bow_word_frequency.get((word, 0), 0) + features[0,2]\n",
    "            return features\n",
    "\n",
    "        def sigmoid(z): \n",
    "            h = 1 / (1+ np.exp(-z))\n",
    "            return h\n",
    "\n",
    "        def predict_tweet(x, theta_ns):\n",
    "            y_pred = sigmoid(np.dot(x, theta_ns))   \n",
    "            return y_pred\n",
    "\n",
    "        def extract_features_torch(processed_tweet, bow_word_frequency):\n",
    "            features = torch.zeros((1,3))\n",
    "            features[0,0] = 1\n",
    "            for word in processed_tweet:\n",
    "                features[0,1] = bow_word_frequency.get((word, 1), 0) + features[0,1]\n",
    "                features[0,2] = bow_word_frequency.get((word, 0), 0) + features[0,2]\n",
    "            return features\n",
    "\n",
    "        def sigmoid_torch(z):\n",
    "            h = 1 / (1+ torch.exp(-z))   \n",
    "            return h\n",
    "\n",
    "        def predict_tweet_torch(x,theta_toc):\n",
    "            y_pred = sigmoid_torch(torch.mm(x,theta_toc))\n",
    "            return y_pred\n",
    "\n",
    "        text_processor = Preprocess()\n",
    "        \n",
    "        bow_word_frequency = joblib.load(open(log_folder + '/bow_word_frequency.pkl','rb'))  \n",
    "        theta_ns = joblib.load(open(logistic_folder + '/logistic.pkl','rb'))\n",
    "        clf = joblib.load(open(sklearn_folder + '/sklearn.pkl','rb'))\n",
    "        theta_toc = joblib.load(open(torch_folder + '/torch.pkl','rb'))\n",
    "        grid_svm = joblib.load(open(svm_folder + '/svm.pkl','rb'))\n",
    "        C_theta_ns = joblib.load(open(logistic_folder + '/logistic_C.pkl','rb'))\n",
    "        C_clf = joblib.load(open(sklearn_folder + '/sklearn_C.pkl','rb'))\n",
    "        C_theta_toc = joblib.load(open(torch_folder + '/torch_C.pkl','rb'))\n",
    "        C_grid_svm = joblib.load(open(svm_folder + '/svm_C.pkl','rb'))\n",
    "        \n",
    "        if request.method == 'POST':\n",
    "            message = request.form['message']\n",
    "            data = [message]\n",
    "            data = text_processor.preprocess(data)\n",
    "            \n",
    "            data_o = str(data)\n",
    "            data_o = data_o[2:len(data_o)-2]\n",
    "\n",
    "            vect = np.zeros((1, 3))\n",
    "            for index, tweet in enumerate(data):\n",
    "                vect[index, :] = extract_features(tweet, bow_word_frequency)\n",
    "            predicted_probs_np = predict_tweet(vect, theta_ns)\n",
    "            my_prediction_np = np.where(predicted_probs_np > 0.5, 1, 0)\n",
    "            C_my_prediction_np = np.where(predicted_probs_np > 0.5, 1, 0)\n",
    "\n",
    "            my_prediction_skl = clf.predict(vect)\n",
    "            my_prediction_svm = grid_svm.predict(vect)\n",
    "            C_my_prediction_skl = C_clf.predict(vect)\n",
    "            C_my_prediction_svm = C_grid_svm.predict(vect)\n",
    "\n",
    "            vect_Tensor = torch.zeros((1, 3))\n",
    "            for index, tweet in enumerate(data):\n",
    "                vect_Tensor[index, :] = extract_features_torch(\n",
    "                    tweet, bow_word_frequency)\n",
    "            predicted_probs_toc = predict_tweet_torch(vect_Tensor, theta_toc)\n",
    "            my_prediction_toc = torch.where(predicted_probs_toc > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "            C_my_prediction_toc = torch.where(predicted_probs_toc > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "            \n",
    "                               \n",
    "                               \n",
    "        return render_template('home.html',\n",
    "                                message = message,\n",
    "                                data = data_o,\n",
    "                                my_prediction_np = my_prediction_np,\n",
    "                                my_prediction_skl = my_prediction_skl,\n",
    "                                my_prediction_toc = my_prediction_toc,\n",
    "                                my_prediction_svm = my_prediction_svm\n",
    "                                C_my_prediction_np = C_my_prediction_np,\n",
    "                                C_my_prediction_skl = C_my_prediction_skl,\n",
    "                                C_my_prediction_toc = C_my_prediction_toc,\n",
    "                                C_my_prediction_svm = C_my_prediction_svm)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        \n",
    "        app.run(debug=True,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Twitter nltk pipeline',\n",
    "    description='Writing code by the other way.'\n",
    ")\n",
    "\n",
    "def nltk_pipeline():\n",
    "    \n",
    "    log_folder = '/information'\n",
    "    pvc_name = \"twitter-5000\"\n",
    "\n",
    "    image = \"ben9053125/mix-final2:latest\"\n",
    "    \n",
    "    vop = dsl.VolumeOp(\n",
    "        name=pvc_name,\n",
    "        resource_name=\"twitter-5000\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWM\n",
    "    )\n",
    "    \n",
    "    dowload_op = func_to_container_op(\n",
    "        func = twitter_sample_dowload_and_preprocess,\n",
    "        base_image = image,\n",
    "    )\n",
    "    C_dowload_op = func_to_container_op(\n",
    "        func = cornell_dowload_and_preprocess,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    numpy_op = func_to_container_op(\n",
    "        func = numpy_process,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    sklearn_op = func_to_container_op(\n",
    "        func = sklearn_logistic,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    logistic_op = func_to_container_op(\n",
    "        func = logistic,\n",
    "        base_image = image,\n",
    "    )\n",
    "    sklearn_svm_op = func_to_container_op(\n",
    "        func = sklearn_svm,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    torch_op = func_to_container_op(\n",
    "        func = torch_process_logistic,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    accuracy_op = func_to_container_op(\n",
    "        func = accuracy,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    http_op = func_to_container_op(\n",
    "        func = http_port,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    dowload_task = dowload_op(log_folder).add_pvolumes({ log_folder:vop.volume, })\n",
    "    C_dowload_task = C_dowload_op(log_folder).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    numpy_task = numpy_op(dowload_task.outputs['logdir'],C_dowload_task.outputs['logdir2']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    sklearn_task = sklearn_op(\n",
    "                                numpy_task.outputs['logdir'],\n",
    "                                numpy_task.outputs['numpydir']\n",
    "    ).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    sklearn_svm_task = sklearn_svm_op(\n",
    "                                numpy_task.outputs['logdir'],\n",
    "                                numpy_task.outputs['numpydir']\n",
    "    ).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    \n",
    "    logistic_task = logistic_op(\n",
    "                                numpy_task.outputs['logdir'],\n",
    "                                numpy_task.outputs['numpydir']\n",
    "    ).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    torch_task = torch_op(\n",
    "                                dowload_task.outputs['logdir'],\n",
    "                                C_dowload_task.outputs['logdir2']\n",
    "    ).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    accuracy_task = accuracy_op(\n",
    "                        sklearn_task.outputs['sklearnscore'],\n",
    "                        sklearn_svm_task.outputs['svm_score'],\n",
    "                        logistic_task.outputs['logisticscore'],\n",
    "                        torch_task.outputs['torchscore'],\n",
    "                        sklearn_task.outputs['C_sklearnscore'],\n",
    "                        sklearn_svm_task.outputs['C_svm_score'],\n",
    "                        logistic_task.outputs['C_logisticscore'],\n",
    "                        torch_task.outputs['C_torchscore']\n",
    "    )\n",
    "        \n",
    "    http_task = http_op(\n",
    "                        sklearn_task.outputs['logdir'],\n",
    "                        sklearn_task.outputs['sklearndir'],\n",
    "                        sklearn_svm_task.outputs['svmdir'],\n",
    "                        logistic_task.outputs['logisticdir'],\n",
    "                        torch_task.outputs['torchdir']\n",
    "    ).add_pvolumes({ log_folder:vop.volume, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(nltk_pipeline, 'twitter-5000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to ./information...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./information...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "7it [00:00, 64.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentiment GOOD total samples 1000\n",
      "negative sentiment  Bad total samples 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:15, 62.51it/s]\n",
      "1000it [00:14, 70.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X_tweet (1600,), test_X_tweet (400,)\n",
      "train_Y (1600,), test_Y (400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to ./information...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./information...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "335it [00:00, 3348.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentiment GOOD total samples 5000\n",
      "negative sentiment  Bad total samples 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:01, 3146.09it/s]\n",
      "5000it [00:01, 3200.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X_tweet (8000,), test_X_tweet (2000,)\n",
      "train_Y (8000,), test_Y (2000,)\n",
      "train_X (8000, 3), test_X (2000, 3)\n",
      "C_train_X (1600, 3), C_test_X (400, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./information', './information/numpy/svm', 0.99, 0.7575]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cornell_dowload_and_preprocess('./information')\n",
    "twitter_sample_dowload_and_preprocess('./information')\n",
    "numpy_process('./information','./information')\n",
    "sklearn_svm('./information','./information/numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
